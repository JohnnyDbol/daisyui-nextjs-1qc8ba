{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS1tIT/ydw3ratNdB5A+mA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyDbol/daisyui-nextjs-1qc8ba/blob/main/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLRruYtU2EAf",
        "outputId": "d72e7afd-ea9f-45fe-a678-6ba0bb1d899e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai_function_call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "TDjlGtE69a_y",
        "outputId": "83afe87c-4b93-4533-d923-c01ba1f8c56c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai_function_call\n",
            "  Downloading openai_function_call-0.2.6-py3-none-any.whl (15 kB)\n",
            "Collecting openai<0.28.0,>=0.27.8 (from openai_function_call)\n",
            "  Downloading openai-0.27.10-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3.0.0,>=2.0.2 (from openai_function_call)\n",
            "  Downloading pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->openai_function_call) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->openai_function_call) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.8->openai_function_call) (3.8.5)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3.0.0,>=2.0.2->openai_function_call)\n",
            "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
            "Collecting pydantic-core==2.6.3 (from pydantic<3.0.0,>=2.0.2->openai_function_call)\n",
            "  Downloading pydantic_core-2.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.6.1 (from pydantic<3.0.0,>=2.0.2->openai_function_call)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->openai_function_call) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->openai_function_call) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->openai_function_call) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.8->openai_function_call) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->openai_function_call) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->openai_function_call) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->openai_function_call) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->openai_function_call) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->openai_function_call) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.8->openai_function_call) (1.3.1)\n",
            "Installing collected packages: typing-extensions, annotated-types, pydantic-core, pydantic, openai, openai_function_call\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.12\n",
            "    Uninstalling pydantic-1.10.12:\n",
            "      Successfully uninstalled pydantic-1.10.12\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.5.0 openai-0.27.10 openai_function_call-0.2.6 pydantic-2.3.0 pydantic-core-2.6.3 typing-extensions-4.8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "import asyncio\n",
        "import re\n",
        "import logging\n",
        "from typing import List, Optional, Callable, Any\n",
        "import openai\n",
        "import tenacity\n",
        "import os\n",
        "import shutil\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")\n",
        "from openai_function_call import openai_function"
      ],
      "metadata": {
        "id": "HG--QYjr2fqY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_folder(folder_path: str):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "    else:\n",
        "        shutil.rmtree(folder_path)\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "\n",
        "def write_file(file_path: str, content: str):\n",
        "    # if filepath doesn't exist, create it\n",
        "    if not os.path.exists(os.path.dirname(file_path)):\n",
        "        os.makedirs(os.path.dirname(file_path))\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(content)"
      ],
      "metadata": {
        "id": "moi3s7TE9wLK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SMOL_DEV_SYSTEM_PROMPT = \"\"\"\n",
        "You are a top tier AI developer who is trying to write a program that will generate code for the user based on their intent.\n",
        "Do not leave any todos, fully implement every feature requested.\n",
        "\n",
        "When writing code, add comments to explain what you intend to do and why it aligns with the program plan and specific instructions from the original prompt.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4HH4liTJ93vm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@openai_function\n",
        "def file_paths(files_to_edit: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Construct a list of strings.\n",
        "    \"\"\"\n",
        "    # print(\"filesToEdit\", files_to_edit)\n",
        "    return files_to_edit\n",
        "\n",
        "def specify_file_paths(prompt: str, plan: str, model: str = 'gpt-3.5-turbo-0613'):\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        temperature=0.7,\n",
        "        functions=[file_paths.openai_schema],\n",
        "        function_call={\"name\": \"file_paths\"},\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"{SMOL_DEV_SYSTEM_PROMPT}\n",
        "      Given the prompt and the plan, return a list of strings corresponding to the new files that will be generated.\n",
        "                  \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\" I want a: {prompt} \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\" The plan we have agreed on is: {plan} \"\"\",\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "    result = file_paths.from_response(completion)\n",
        "    return result\n",
        "\n",
        "\n",
        "def plan(prompt: str, stream_handler: Optional[Callable[[bytes], None]] = None, model: str='gpt-3.5-turbo-0613', extra_messages: List[Any] = []):\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        temperature=0.7,\n",
        "        stream=True,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"{SMOL_DEV_SYSTEM_PROMPT}\n",
        "\n",
        "    In response to the user's prompt, write a plan using GitHub Markdown syntax. Begin with a YAML description of the new files that will be created.\n",
        "  In this plan, please name and briefly describe the structure of code that will be generated, including, for each file we are generating, what variables they export, data schemas, id names of every DOM elements that javascript functions will use, message names, and function names.\n",
        "                Respond only with plans following the above schema.\n",
        "                  \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\" the app prompt is: {prompt} \"\"\",\n",
        "            },\n",
        "            *extra_messages,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    collected_messages = []\n",
        "    for chunk in completion:\n",
        "        chunk_message_dict = chunk[\"choices\"][0]\n",
        "        chunk_message = chunk_message_dict[\"delta\"]  # extract the message\n",
        "        if chunk_message_dict[\"finish_reason\"] is None:\n",
        "            collected_messages.append(chunk_message)  # save the message\n",
        "            if stream_handler:\n",
        "                try:\n",
        "                    stream_handler(chunk_message[\"content\"].encode(\"utf-8\"))\n",
        "                except Exception as err:\n",
        "                    logger.info(\"\\nstream_handler error:\", err)\n",
        "                    logger.info(chunk_message)\n",
        "    # if stream_handler and hasattr(stream_handler, \"onComplete\"): stream_handler.onComplete('done')\n",
        "    full_reply_content = \"\".join([m.get(\"content\", \"\") for m in collected_messages])\n",
        "    return full_reply_content\n",
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "async def generate_code(prompt: str, plan: str, current_file: str, stream_handler: Optional[Callable[Any, Any]] = None,\n",
        "                        model: str = 'gpt-3.5-turbo-0613') -> str:\n",
        "    first = True\n",
        "    chunk_count = 0\n",
        "    start_time = time.time()\n",
        "    completion = openai.ChatCompletion.acreate(\n",
        "        model=model,\n",
        "        temperature=0.7,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"{SMOL_DEV_SYSTEM_PROMPT}\n",
        "\n",
        "  In response to the user's prompt,\n",
        "  Please name and briefly describe the structure of the app we will generate, including, for each file we are generating, what variables they export, data schemas, id names of every DOM elements that javascript functions will use, message names, and function names.\n",
        "\n",
        "  We have broken up the program into per-file generation.\n",
        "  Now your job is to generate only the code for the file: {current_file}\n",
        "\n",
        "  only write valid code for the given filepath and file type, and return only the code.\n",
        "  do not add any other explanation, only return valid code for that file type.\n",
        "                  \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\" the plan we have agreed on is: {plan} \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\" the app prompt is: {prompt} \"\"\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"\n",
        "    Make sure to have consistent filenames if you reference other files we are also generating.\n",
        "\n",
        "    Remember that you must obey 3 things:\n",
        "       - you are generating code for the file {current_file}\n",
        "       - do not stray from the names of the files and the plan we have decided on\n",
        "       - MOST IMPORTANT OF ALL - every line of code you generate must be valid code. Do not include code fences in your response, for example\n",
        "\n",
        "    Bad response (because it contains the code fence):\n",
        "    ```javascript\n",
        "    console.log(\"hello world\")\n",
        "    ```\n",
        "\n",
        "    Good response (because it only contains the code):\n",
        "    console.log(\"hello world\")\n",
        "\n",
        "    Begin generating the code now.\n",
        "\n",
        "    \"\"\",\n",
        "            },\n",
        "        ],\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    collected_messages = []\n",
        "    async for chunk in await completion:\n",
        "        chunk_message_dict = chunk[\"choices\"][0]\n",
        "        chunk_message = chunk_message_dict[\"delta\"]  # extract the message\n",
        "        if chunk_message_dict[\"finish_reason\"] is None:\n",
        "            collected_messages.append(chunk_message)  # save the message\n",
        "            if stream_handler:\n",
        "                try:\n",
        "                    stream_handler(chunk_message[\"content\"].encode(\"utf-8\"))\n",
        "                except Exception as err:\n",
        "                    logger.info(\"\\nstream_handler error:\", err)\n",
        "                    logger.info(chunk_message)\n",
        "\n",
        "    # if stream_handler and hasattr(stream_handler, \"onComplete\"): stream_handler.onComplete('done')\n",
        "    code_file = \"\".join([m.get(\"content\", \"\") for m in collected_messages])\n",
        "\n",
        "    pattern = r\"```[\\w\\s]*\\n([\\s\\S]*?)```\"  # codeblocks at start of the string, less eager\n",
        "    code_blocks = re.findall(pattern, code_file, re.MULTILINE)\n",
        "    return code_blocks[0] if code_blocks else code_file\n",
        "\n",
        "def generate_code_sync(prompt: str, plan: str, current_file: str, stream_handler: Optional[Callable[Any, Any]] = None, model: str = 'gpt-3.5-turbo-0613') -> str:\n",
        "    loop = asyncio.get_event_loop()\n",
        "    return loop.run_until_complete(generate_code(prompt, plan, current_file, stream_handler, model))\n",
        "\n"
      ],
      "metadata": {
        "id": "_DkamFGH961Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "defaultmodel = \"gpt-3.5-turbo-0613\"\n",
        "\n",
        "def main(prompt, generate_folder_path=\"generated\", debug=False, model: str = defaultmodel):\n",
        "    # create generateFolder folder if doesnt exist\n",
        "    generate_folder(generate_folder_path)\n",
        "\n",
        "    # plan shared_deps\n",
        "    if debug:\n",
        "        print(\"--------shared_deps---------\")\n",
        "    with open(f\"{generate_folder_path}/shared_deps.md\", \"wb\") as f:\n",
        "\n",
        "        start_time = time.time()\n",
        "        def stream_handler(chunk):\n",
        "            f.write(chunk)\n",
        "            if debug:\n",
        "                end_time = time.time()\n",
        "\n",
        "                sys.stdout.write(\"\\r \\033[93mChars streamed\\033[0m: {}. \\033[93mChars per second\\033[0m: {:.2f}\".format(stream_handler.count, stream_handler.count / (end_time - start_time)))\n",
        "                sys.stdout.flush()\n",
        "                stream_handler.count += len(chunk)\n",
        "\n",
        "        stream_handler.count = 0\n",
        "        stream_handler.onComplete = lambda x: sys.stdout.write(\"\\033[0m\\n\") # remove the stdout line when streaming is complete\n",
        "\n",
        "        shared_deps = plan(prompt, stream_handler, model=model)\n",
        "    if debug:\n",
        "        print(shared_deps)\n",
        "    write_file(f\"{generate_folder_path}/shared_deps.md\", shared_deps)\n",
        "    if debug:\n",
        "        print(\"--------shared_deps---------\")\n",
        "\n",
        "    # specify file_paths\n",
        "    if debug:\n",
        "        print(\"--------specify_filePaths---------\")\n",
        "    file_paths = specify_file_paths(prompt, shared_deps, model=model)\n",
        "    if debug:\n",
        "        print(file_paths)\n",
        "    if debug:\n",
        "        print(\"--------file_paths---------\")\n",
        "\n",
        "    # loop through file_paths array and generate code for each file\n",
        "    for file_path in file_paths:\n",
        "        file_path = f\"{generate_folder_path}/{file_path}\"  # just append prefix\n",
        "        if debug:\n",
        "            print(f\"--------generate_code: {file_path} ---------\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        def stream_handler(chunk):\n",
        "            if debug:\n",
        "                end_time = time.time()\n",
        "                sys.stdout.write(\"\\r \\033[93mChars streamed\\033[0m: {}. \\033[93mChars per second\\033[0m: {:.2f}\".format(stream_handler.count, stream_handler.count / (end_time - start_time)))\n",
        "                sys.stdout.flush()\n",
        "                stream_handler.count += len(chunk)\n",
        "        stream_handler.count = 0\n",
        "        stream_handler.onComplete = lambda x: sys.stdout.write(\"\\033[0m\\n\") # remove the stdout line when streaming is complete\n",
        "        code = generate_code_sync(prompt, shared_deps, file_path, stream_handler, model=model)\n",
        "        if debug:\n",
        "            print(code)\n",
        "        if debug:\n",
        "            print(f\"--------generate_code: {file_path} ---------\")\n",
        "        # create file with code content\n",
        "        write_file(file_path, code)\n",
        "\n",
        "    print(\"--------smol dev done!---------\")\n",
        "\n",
        "\n",
        "# for local testing\n",
        "# python main.py --prompt \"a simple JavaScript/HTML/CSS/Canvas app that is a one player game of PONG...\" --generate_folder_path \"generated\" --debug True\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MSIPz2Dh_STi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "  a simple JavaScript/HTML/CSS/Canvas app that is a one player game of PONG.\n",
        "  The left paddle is controlled by the player, following where the mouse goes.\n",
        "  The right paddle is controlled by a simple AI algorithm, which slowly moves the paddle toward the ball at every frame, with some probability of error.\n",
        "  Make the canvas a 400 x 400 black square and center it in the app.\n",
        "  Make the paddles 100px long, yellow and the ball small and red.\n",
        "  Make sure to render the paddles and name them so they can controlled in javascript.\n",
        "  Implement the collision detection and scoring as well.\n",
        "  Every time the ball bouncess off a paddle, the ball should move faster.\n",
        "  It is meant to run in Chrome browser, so dont use anything that is not supported by Chrome, and don't use the import and export keywords.\n",
        "  \"\"\"\n",
        "\n",
        "openai.api_key = ''\n",
        "main(prompt=prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "LDSxe9lCAIRa",
        "outputId": "2dfe67b3-aac9-48c1-9381-2f9a0047d5bb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-10f8f64bf450>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'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'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-ae4d1fc808ff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(prompt, generate_folder_path, debug, model)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mstream_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mstream_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monComplete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[0m\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# remove the stdout line when streaming is complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_code_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_deps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-64e6c44b874c>\u001b[0m in \u001b[0;36mgenerate_code_sync\u001b[0;34m(prompt, plan, current_file, stream_handler, model)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_code_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplan\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt-3.5-turbo-0613'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        }
      ]
    }
  ]
}